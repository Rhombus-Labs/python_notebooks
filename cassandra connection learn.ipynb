{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.databricks:spark-avro_2.11:4.0.0,org.apache.hadoop:hadoop-aws:2.7.3 --conf spark.cassandra.connection.host=46.4.115.158 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up spark session\n",
    "import pyspark as py\n",
    "spark = py.sql.SparkSession.builder.master('local[1]').appName('pyspark Jupyter learning').config('spark.cassandra.connection.host','5.9.58.93').getOrCreate()\n",
    "spark.conf.set('spark.executor.memory','1g')\n",
    "spark.conf.set('spark.driver.memory','2g')\n",
    "spark.conf.set('spark.cassandra.connection.host','5.9.58.93')\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAITKKPYA35KQ4M2LQ\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from cassandra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTablefromKeyspace(tbl_name,keyspace):\n",
    "    rawTbldf = sqlcontext.read.format(\"org.apache.spark.sql.cassandra\").options(table=tbl_name,keyspace=keyspace).load()\n",
    "    return rawTbldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----------+----------+----------+------------+------------+\n",
      "|ih_no|ih_mrp|  ih_name|ih_pur_tax|ih_purrate|ih_sel_tax|ih_sell_rate|ih_tot_stock|\n",
      "+-----+------+---------+----------+----------+----------+------------+------------+\n",
      "|    1|  35.0|    apple|         3|      20.0|         3|        30.0|         100|\n",
      "|    4| 100.0|pineapple|         4|      15.0|         3|        19.0|         100|\n",
      "|    5|  50.0|    onion|         5|      34.0|         3|        36.0|          85|\n",
      "|   10|  68.0|   tomato|         4|      34.0|         3|        50.0|          80|\n",
      "|    8|  75.0|    pepsi|         2|      14.0|         3|        30.0|          45|\n",
      "+-----+------+---------+----------+----------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemHdr = loadTablefromKeyspace(\"item_hdr\",\"danny\")\n",
    "itemHdr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|tm_id|tm_tax_per|\n",
      "+-----+----------+\n",
      "|    1|         0|\n",
      "|    4|        12|\n",
      "|    5|        18|\n",
      "|    8|       188|\n",
      "|    2|         3|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxMasterDf = loadTablefromKeyspace(\"tax_master\",\"danny\")\n",
    "taxMasterDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data to be added to the tax table in cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|tm_id|tm_tax_per|\n",
      "+-----+----------+\n",
      "|    3|        40|\n",
      "|    7|        99|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "newTax = [(3,40),(7,99)]\n",
    "newTaxRdd = sc.parallelize(newTax)\n",
    "newTaxRddRow = newTaxRdd.map(lambda x: Row(tm_id = x[0],tm_tax_per = x[1]))\n",
    "newTaxDf = sqlcontext.createDataFrame(newTaxRddRow)\n",
    "newTaxDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the new data into the tax table in cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|tm_id|tm_tax_per|\n",
      "+-----+----------+\n",
      "|    1|         0|\n",
      "|    4|        12|\n",
      "|    5|        18|\n",
      "|    8|       188|\n",
      "|    2|         3|\n",
      "|    7|        99|\n",
      "|    6|        40|\n",
      "|    3|         5|\n",
      "|    3|        40|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newTaxDf.write.format('org.apache.spark.sql.cassandra').mode('append').options(table='tax_master',keyspace ='danny').save()\n",
    "loadTablefromKeyspace('tax_master','danny').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV as Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadharDf = spark.read.format('csv').option('header','true').option('inferSchema','true').option('delimiter',',').load('hdfs://5.9.58.93:8020/demo/cmdlearn/aadhar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadharDf.registerTempTable('aadhar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------+------+\n",
      "|  ageGrp|        State|Sub District|Gender|\n",
      "+--------+-------------+------------+------+\n",
      "|Age 0-20|Uttar Pradesh|        Meja|     F|\n",
      "|Age 0-20|Uttar Pradesh| Robertsganj|     M|\n",
      "|Age 0-20|Uttar Pradesh|   Sultanpur|     F|\n",
      "|Age 0-20|Uttar Pradesh|      Shamli|     M|\n",
      "|Age 0-20|Uttar Pradesh|    Sahjanwa|     M|\n",
      "+--------+-------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adhrWithAgeGrp = sqlcontext.sql(\"select  case when Age >= 85 then 'Age >= 85' when Age < 85 and Age >= 50 then 'Age 50-85' when Age < 50 and Age >= 20 then 'Age 20-50' else 'Age 0-20' end as ageGrp,State,`Sub District`,Gender from aadhar\")\n",
    "adhrWithAgeGrp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrWithAgeGrp.registerTempTable('ageGrouped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adhrAgeGrpCnt = sqlcontext.sql(\"select ageGrp,count(*) as cnt from ageGrouped group by ageGrp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing csv file into HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrAgeGrpCnt.write.csv('hdfs://5.9.58.93:8020/aakash/aadharGrpCnt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing dataframe as parquet file into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrAgeGrpCnt.write.save('hdfs://5.9.58.93:8020/aakash/aadharcnt.parquet',format='parquet',mode = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrAgeGrpCnt.write.parquet('hdfs://5.9.58.93:8020/aakash/adhrcnt.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|   ageGrp|   cnt|\n",
      "+---------+------+\n",
      "|Age 50-85| 41214|\n",
      "|Age 20-50|139093|\n",
      "|Age >= 85|   313|\n",
      "| Age 0-20|260198|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('parquet').load('hdfs://5.9.58.93:8020/aakash/adhrcnt.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|   ageGrp|   cnt|\n",
      "+---------+------+\n",
      "|Age 50-85| 41214|\n",
      "|Age 20-50|139093|\n",
      "|Age >= 85|   313|\n",
      "| Age 0-20|260198|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('hdfs://5.9.58.93:8020/aakash/adhrcnt.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data frame as Avro file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrAgeGrpCnt.write.format('com.databricks.spark.avro').save('hdfs://5.9.58.93:8020/aakash/aadharAvro.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrWithAgeGrp = adhrWithAgeGrp.withColumnRenamed('sub district','sub_district')\n",
    "adhrWithAgeGrp.write.format('com.databricks.spark.avro').save('hdfs://5.9.58.93:8020/aakash/adhrWthAgeGrp.avro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading avro file as Dataframe from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|   ageGrp|   cnt|\n",
      "+---------+------+\n",
      "|Age 50-85| 41214|\n",
      "|Age 20-50|139093|\n",
      "| Age 0-20|260198|\n",
      "|Age >= 85|   313|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlcontext.read.format('com.databricks.spark.avro').load('hdfs://5.9.58.93:8020/aakash/aadharAvro.avro').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|   ageGrp|   cnt|\n",
      "+---------+------+\n",
      "|Age 50-85| 41214|\n",
      "|Age 20-50|139093|\n",
      "| Age 0-20|260198|\n",
      "|Age >= 85|   313|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adhrAvro = spark.read.format('com.databricks.spark.avro').load('hdfs://5.9.58.93:8020/aakash/aadharAvro.avro')\n",
    "adhrAvro.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from S3 server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+---+\n",
      "|Age 0-20|Uttar Pradesh|       Meja|  F|\n",
      "+--------+-------------+-----------+---+\n",
      "|Age 0-20|Uttar Pradesh|Robertsganj|  M|\n",
      "|Age 0-20|Uttar Pradesh|  Sultanpur|  F|\n",
      "|Age 0-20|Uttar Pradesh|     Shamli|  M|\n",
      "|Age 0-20|Uttar Pradesh|   Sahjanwa|  M|\n",
      "|Age 0-20|Uttar Pradesh|     Pindra|  M|\n",
      "+--------+-------------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adhrAgeGrpS3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrWithAgeGrp = adhrWithAgeGrp.withColumnRenamed('sub district','sub_district')\n",
    "adhrWithAgeGrp.write.format('com.databricks.spark.avro').save('s3n://todaynodejs/adhrWthAgeGrp.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------+------+\n",
      "|  ageGrp|        State|sub_district|Gender|\n",
      "+--------+-------------+------------+------+\n",
      "|Age 0-20|Uttar Pradesh|        Meja|     F|\n",
      "|Age 0-20|Uttar Pradesh| Robertsganj|     M|\n",
      "|Age 0-20|Uttar Pradesh|   Sultanpur|     F|\n",
      "|Age 0-20|Uttar Pradesh|      Shamli|     M|\n",
      "|Age 0-20|Uttar Pradesh|    Sahjanwa|     M|\n",
      "+--------+-------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adhrfroms3 = spark.read.format('com.databricks.spark.avro').load('s3n://todaynodejs/adhrWthAgeGrp.avro')\n",
    "adhrfroms3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing parquet file from S3 server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrWithAgeGrp.write.format('parquet').save('s3n://todaynodejs/adhragegrp.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the parquet file from S3 server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------+------+\n",
      "|  ageGrp|        State|sub_district|Gender|\n",
      "+--------+-------------+------------+------+\n",
      "|Age 0-20|Uttar Pradesh|        Meja|     F|\n",
      "|Age 0-20|Uttar Pradesh| Robertsganj|     M|\n",
      "|Age 0-20|Uttar Pradesh|   Sultanpur|     F|\n",
      "|Age 0-20|Uttar Pradesh|      Shamli|     M|\n",
      "|Age 0-20|Uttar Pradesh|    Sahjanwa|     M|\n",
      "+--------+-------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('parquet').load('s3n://todaynodejs/adhragegrp.parquet').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the dataframe partitioned by a particular column as avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrWithAgeGrp.write.partitionBy('ageGrp').format('com.databricks.spark.avro').save('s3n://todaynodejs/adhrpartitionavro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reading the partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrpartRead = spark.read.format('com.databricks.spark.avro').load('s3n://todaynodejs/adhrpartitionavro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440818 440818\n"
     ]
    }
   ],
   "source": [
    "print(adhrpartRead.count(),adhrWithAgeGrp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing dataframe partitioned as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhrWithAgeGrp.write.partitionBy('ageGrp').format('parquet').save('s3n://todaynodejs/adhrpartitionParq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------+--------+\n",
      "|        State|sub_district|Gender|  ageGrp|\n",
      "+-------------+------------+------+--------+\n",
      "|Uttar Pradesh|        Meja|     F|Age 0-20|\n",
      "|Uttar Pradesh| Robertsganj|     M|Age 0-20|\n",
      "|Uttar Pradesh|   Sultanpur|     F|Age 0-20|\n",
      "|Uttar Pradesh|      Shamli|     M|Age 0-20|\n",
      "|Uttar Pradesh|    Sahjanwa|     M|Age 0-20|\n",
      "+-------------+------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adhrparqRead = spark.read.format('parquet').load('s3n://todaynodejs/adhrpartitionParq')\n",
    "adhrparqRead.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440818 440818\n"
     ]
    }
   ],
   "source": [
    "print(adhrWithAgeGrp.count(),adhrparqRead.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
